# -*- coding: utf-8 -*-
#"""Bot1.ipynb

#Automatically generated by Colaboratory.

#Original file is located at
#    https://colab.research.google.com/drive/1mqp7pTLkPPDX4Y9kj6_4OSnZGBtZrLi8


print("Вас приветсвует чат-бот Bot1")
# 1 Вопрос = 1 Ответ // Чат-бот v1

question = "Как дела?" # Вопрос чат-боту
if question == "Как дела?":
  print("Дела нормально")

if question == "Как тебя зовут?":
  print("Меня зовут чат-бот")

# 1 Искать неточное совпадение
# 2 Предусмотреть разные ответы
# 3 Создать базу вопросов и ответов

#from nltk.metrics.aline import R
# 1 Искать неточное совпадение
#import nltk
#import re
#import random

#def filter(text): #Убираем знаки препинания, приводим текст к нижнемсу регистру
# text = text.lower() # Нижний регистр
#  punctuation = R"\W" #Удалить знаки препинания
#  return re.sub(punctuation, "", text)
  

#def matchText(text1, text2): #создаем функцию, которая сравнит два текста
#  text1 = filter(text1)
#  text2 = filter(text2)
#  distance = nltk.edit_distance(text1, text2) # Посчитаем расстояние между текстами
#  average_length = (len(text1) + len(text2)) / 2 #Средняя длина текста
#  return distance / average_length # Отличие текстов в %
 
#question = input() # Вопрос, который задаем боту

# if matchText(question, "Как дела?") < 0.4:
# print("Дела нормально")

# Список вопросов и ответов
#database = [
#    {
#        "question": "Как дела",
#        "answer": ["Дела отлично", "Дела средне", "Ок"],
#    },
#    {   "question": "Как тебя зовут",
#        "answer": ["Меня зовут бот", "Как тебе больше нравится", "Тебя-то самого как зовут"],
#    },
#    {   "question": "Как погода",
#        "answer": ["Отлично", "Сыро", "Солнечно"],
#    },
#    {   "question": "Какой сегодня день недели",
#        "answer": ["Понедельник", "Вторник", "Среда"],
#    },
#    {   "question": "Который сейчас час",
#        "answer": ["Половина третьего", "Полдень", "Счастливые часов не наблюдают"],
#    },
#    {   "question": "Сколько тебе лет",
#        "answer": ["13", "Такие вопросы задавать не прилично", "41"],
#   },
#    {   "question": "Какие планы на вечер",
#        "answer": ["Чай, печенки, сериал", "Пойду гулять", "Буду спать"],
#    },
#    {   "question": "Сходим в кино",
#        "answer": ["Не сегодня", "Конечно", "Надо подумать"],
#    },
#    {   "question": "Как настроение",
#        "answer": ["Отлично", "Супер", "Так себе"],
#    },
#    {   "question": "Какое сейчас время года",
#        "answer": ["Осень", "Весна", "Какое предпочитаете?"],
#    },
#    {   "question": "Где я",
#        "answer": ["Дома", "В кафе", "Да вы, батенька, заблудились"],
#    },
#    {   "question": "Ты кто",
#        "answer": ["Я - пользователь", "Да кто его знает", "Какая разница"]
#    }
#]
#
#for pair in database:
#  if matchText(question, pair["question"]) < 0.4:
#    answer = random.choice(pair["answer"])
#    print(answer)

# BOT_CONFIG = {
#     "intents": { # Намерения пользователя
#         "hello": { # Поздороваться
#             "examples": ["Привет", "Добрый день", "Шалом", "Здрасте", "Здравствуйте", "Доброе время суток"],
#             "responses": ["Привет, человек", "И вам здрасте", "Йоу"],
#         },
#         "bye": {
#             "examples": ["Пока", "До свидания", "Досвидос", "Прощай"],
#             "responses": ["Счастливо", "До свидания", "Если что - возвращайтесь"]
#         },
#         "how_are_you": {
#             "examples": ["Как дела", "Что делаешь", "Какие делища", "Как поживаешь", "Чо как"],
#             "responses": ["Маюсь фигней", "Учу Python", "Смотрю вебинары Скиллбокс"],      
#         },
#     },
#     "failure_phrases": ["Йа ничо ни понил", "Что-то непонятно", "Я всего лишь бот, сформулируйте попроще"]
# }

import json # Импорт библиотеки JSON
config_file = open("config.json", "r") # Открытие файла
BOT_CONFIG = json.load(config_file) # Преобразование из JSON в ст

import re
import nltk
import random

def normalize(text): # Создаем функцию, которая выкинет знаки препинания и приведет текст к нижнему регистру
  text = text.lower() # "ПРиВЕт" => "привет"
  # Удалять из текста знаки препинания с помощью "Regular Expressions"
  punctuation = r"[^\w\s]" # выражение позволяет удалить все знаки препинания
  # ^ - "все кроме"
  # \w - "буквы"
  # \s - "пробелы"

  # Старое выражение = \W = "все кроме букв"
  return re.sub(punctuation, "", text) # Заменяем все что попадает под шаблон punctuation на пустую строку "" в тексте text

def isMatching(text1, text2): # Создаем функцию, которая посчитает похожи ли два текста
  text1 = normalize(text1)
  text2 = normalize(text2)
  distance = nltk.edit_distance(text1, text2) # Посчитаем расстояние между текстами (насколько они отличается)
  average_length = (len(text1) + len(text2)) / 2 # Посчитаем среднюю длину текстов
  return distance / average_length < 0.4

def getIntent(text): # Понимать намерение по тексту
  all_intents = BOT_CONFIG["intents"]
  for name, data in all_intents.items(): # Пройти по всем намерениям и положить название в name, и остальное в переменную data
    for example in data["examples"]: # Пройти по всем примерам этого интента, и положить текст в переменную example
      if isMatching(text, example): # Если текст совпадает с примером
        return name

def getAnswer(intent):
  responses = BOT_CONFIG["intents"][intent]["responses"]
  return random.choice(responses)

def bot(text): # Функция = Бот
  # Пытаемся опеределить намерение  
  intent = getIntent(text)

  if not intent: # Если намерение не найдено
     # ToDO: подключить модель машинного обучения (классификатор текстов)
     test = vectorizer.transform([text])
     intent = model.predict(test)[0] # По Х предсказать у, т.е. классифицировать 

  print("Intent =", intent)

  if intent: # Если намерение найдено - выдать ответ
    return getAnswer(intent)

  # Заглушка
  failure_phrases = BOT_CONFIG['failure_phrases']
  return random.choice(failure_phrases)


# Обучить модель для классификации текстов
# тексты
X = []
# классы
y = []
# Задача модели = это по "Х" научиться находить "у"
for name, data in BOT_CONFIG["intents"].items():
  for example in data['examples']:
    X.append(example) # Собираем тексты в Х
    y.append(name) # Собираем классы в у

X[-3:]

"""Набор текстов свести к набору чисел = "Векторайзер"

Мама круто мыла раму = [1,2,3,4]

Круто раму мыла мама = [2,4,3,1]

Мыла раму круто мама = [3,4,2,1]

"мама" = 1 "круто" = 2 "мыла" = 3 "раму" = 4
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer() # Можно указать настройки
vectorizer.fit(X) # Передаем набор текстов, чтобы векторайзер их проанализировал

X_vectorized = vectorizer.transform(X) # Трансформируем тексты в вектора (наборы чисел)

#from sklearn.linear_model import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier() # Настройки
model.fit(X_vectorized, y) # Модель научиться по Х определять у
model.score(X_vectorized, y)

test = vectorizer.transform(["ну расскажи ка мне анекдот"])
model.predict(test) # По X предсказать Y

"""# Новый раздел"""
answer=input('?')
if answer=='стоп':
	print('пока пока')
	exit()
else:
	answer=bot(answer)
	print(answer)

